---
title: "p8105_hw2_ww2744"
author: "Wenjie Wu"
output: github_document
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(readxl)
library(haven)
library(dplyr)
library(scales)
```

# Problem 1 

Import the NYC Transit data

```{r}
trans_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "",".")) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
```

- The dataset contains 1863 rows and 19 columns.
- This dataset contains information about subway entrances and exits in New York City. Key variables include the subway line, station name, station latitude/longitude, routes served, entry (whether the entrance allows entry), vending, entrance type (e.g., stairs, escalator), and ADA compliance.

Cleaning steps:

- Load data, take `NA` vaules like: "NA", "", "." as missing values.
- Use janitor::clean_names() to transform all the col names into letters
- Use `select` function to select the columns needed to be anaylzed.
- Use `case_match` to convert `entry` variable to a logical variable.

```{r}
distinct_stations = trans_df |>
  distinct(line, station_name) |>
  nrow() |>
  print()
```

- There are `distinct_stations` distinct stations.

```{r}
ada_compliant_stations = trans_df |>
  filter(ada == TRUE) |>
  distinct(line, station_name) |>
  nrow() |>
  print()
```

- 84 stations are ADA compliant.

```{r}
proportion_non_vending_allow_entry = trans_df |>
  filter(vending == "NO") |>
  summarise(proportion = mean(entry == TRUE, na.rm = TRUE)) |>
  unlist() |>
  percent() |>
  print()
```
- About `r proportion_non_vending_allow_entry` of station entrances/exits without vending machines allow entrance.

```{r}
tidy_data = trans_df |>
  mutate(across(starts_with("Route"), as.character)) |>
  pivot_longer(cols = starts_with("route"), 
               names_to = "route_Number", 
               values_to = "route_Name", 
               values_drop_na = TRUE)

stations_serving_A = tidy_data |>
  filter(route_Name == "A") |>
  distinct(line, station_name) |>
  nrow()|>
  print()

ada_compliant_stations_A = tidy_data |>
  filter(route_Name == "A" & ada == TRUE) |>
  distinct(line, station_name) |>
  nrow() |>
  print()
```

- `r stations_serving_A` distinct stations serve the A train.
- Among them `r ada_compliant_stations_A` are ADA compliant.


# Problem 2

Read and clean the data

Mr. Trash Wheel

```{r, message = FALSE}
Trash_wheel = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    skip = 1,
    na = c("NA", ",", ".")
  ) |>
  select(-c(...15, ...16)) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )
  
```

 Load Professor Trash Wheel and Gwynnda

```{r}
Professor_trash_wheel = 
  read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel",
    skip = 1,
    na = c("NA", ",", ".")
  ) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster))
  

Gwynnda_trash_wheel = 
    read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel",
    skip = 1,
    na = c("NA", ",", ".")
  ) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster))


```

Combine the datasets

```{r}
Trash_wheel = Trash_wheel |> mutate(trash_wheel = "Mr. Trash Wheel",
                      year = as.character(year))
Professor_trash_wheel = Professor_trash_wheel |> mutate(trash_wheel = "Professor Trash Wheel",
                                year = as.character(year))
Gwynnda_trash_wheel = Gwynnda_trash_wheel|>  mutate(trash_wheel = "Gwynnda Trash Wheel",
                              year = as.character(year))


combined_trash_wheel = bind_rows(Trash_wheel, Professor_trash_wheel, Gwynnda_trash_wheel) |>
  select(-c(dumpster)) |>
  relocate(trash_wheel)
```

- The combined dataset of trash collected by Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel contains `r nrow(combined_trash_wheel)` observations. 
- Key variables include weight_tons representing the weight of trash collected per dumpster, the time of collection and specific waste types like plastic_bottles and cigarette_butts. 

```{r}
total_weight_prof = sum(
  filter(combined_trash_wheel, trash_wheel == "Professor Trash Wheel") |>
    select(weight_tons),
  na.rm = TRUE
)
total_weight_gwy = sum(
  filter(combined_trash_wheel, trash_wheel == "Professor Trash Wheel" & month == "June" & year == "2022" ) |>
    select(cigarette_butts), 
    na.rm = TRUE
)
  
```

-  For Professor Trash Wheel, the total weight of trash collected is `r total_weight_prof` tons. Meanwhile, Gwynnda Trash Wheel collected `r total_weight_gwy`cigarette butts in June of 2022.


# Problem 3

Import and clean datasets


```{r}
bakers_df =   read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "",".")) |>
  janitor::clean_names() |>
  separate(col = baker_name, into = c("baker", "baker_last_name"), sep = " ") 

bakes_df = read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "",".", "N/A")) |>
  janitor::clean_names() |>
  mutate(baker = ifelse(baker == '"Jo"', "Joanne", baker))

results_df = read_csv("data/gbb_datasets/results.csv", na = c("NA", "","."), skip = 2) |>
  janitor::clean_names()

viewers_df = read_csv("data/gbb_datasets/viewers.csv", na = c("NA", "",".")) |>
  janitor::clean_names() |>
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewers",
    names_prefix = "series_"
  ) |>
  arrange(series) |>
  relocate(series, episode)
```


Check for completeness and correctness


```{r}
missing_in_bakes = anti_join(bakers_df, bakes_df, 
                              by = c("baker" = "baker", "series"))

missing_in_results = anti_join(bakers_df, results_df, 
                                by = c("baker" = "baker", "series"))

missing_in_results_for_bakes = anti_join(bakes_df, results_df, 
                                          by = c("baker", "series", "episode"))
```


Create a single dataset


```{r}
GBBF_df = left_join(results_df, bakes_df, by = c("series", "episode", "baker")) |>
  left_join(bakers_df, by = c("series", "baker")) |>
  relocate(series, episode, baker, baker_last_name, baker_age, baker_occupation, hometown, signature_bake, technical, result, show_stopper)

write.csv(GBBF_df, file = "data/gbb_datasets/GBBF.csv", row.names = FALSE)
```

Data cleaning process:

- Import datasets, use`Janitor:clean_names` to uniform the column names.
- The `bakes.csv` takes `N/A` as `NA`.
- Check the association of column elements with elements from another tableï¼Œseperate the column  `baker name` in `baker.csv` because the other datasets only use baker's first name. 
- Use `anti_join` to check the completeness and correctness of the dataframes, found that `"Jo"` in `bakes.csv` equals to `Joanne` in `results.csv`, use `mutate` to change it.
- Use `left_join` to merge the three processed dataframes. A `left_join` means all rows from `results_df` are retained, and matching rows from `bakes_df` are added. If no match is found in `bakes_df`, ` NA` will be introduced for the missing data.
- `Relocate` is used to reorder the columns of the resulting dataset, making sure that key columns like series, episode, baker, and related information are positioned in a logical order for easier interpretation and analysis.
- The final dataset have 1136 rows and 11 columns.


Winner of each episode in Seasons 5 through 10

```{r}
winner_df = select(GBBF_df, series, episode, baker, result) |>
  filter(result %in% c("WINNER", "STAR BAKER"), series > 4 ) |>
  print()
```




Viewership Data

```{r}
viewers_df |>
  slice_head(n = 10) |>
  print()
```

```{r}
s1_viewer = viewers_df |>
  filter(series == 1) |>
  pull(viewers) |>
  mean(na.rm = TRUE) |>
  print()


s5_viewer = viewers_df |>
  filter(series == 5) |>
  pull(viewers) |>
  mean(na.rm = TRUE) |>
  print()
```



The average viewship in Season 1 is `r s1_viewer`, Season 5 is `r s5_viewer`.