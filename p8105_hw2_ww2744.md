p8105_hw2_ww2744
================
Wenjie Wu

# Problem 1

Import the NYC Transit data

``` r
trans_df = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "",".")) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(
    entry = case_match(
      entry,
      "YES" ~ TRUE,
      "NO" ~ FALSE
    )
  )
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

- The dataset contains 1863 rows and 19 columns.
- This dataset contains information about subway entrances and exits in
  New York City. Key variables include the subway line, station name,
  station latitude/longitude, routes served, entry (whether the entrance
  allows entry), vending, entrance type (e.g., stairs, escalator), and
  ADA compliance.

Cleaning steps:

- Load data, take `NA` vaules like: “NA”, ““,”.” as missing values.
- Use janitor::clean_names() to transform all the col names into letters
- Use `select` function to select the columns needed to be anaylzed.
- Use `case_match` to convert `entry` variable to a logical variable.

``` r
distinct_stations = trans_df |>
  distinct(line, station_name) |>
  nrow() |>
  print()
```

    ## [1] 465

- There are `distinct_stations` distinct stations.

``` r
ada_compliant_stations = trans_df |>
  filter(ada == TRUE) |>
  distinct(line, station_name) |>
  nrow() |>
  print()
```

    ## [1] 84

- 84 stations are ADA compliant.

``` r
proportion_non_vending_allow_entry = trans_df |>
  filter(vending == "NO") |>
  summarise(proportion = mean(entry == TRUE, na.rm = TRUE)) |>
  unlist() |>
  percent() |>
  print()
```

    ## proportion 
    ##      "38%"

- About 38% of station entrances/exits without vending machines allow
  entrance.

``` r
tidy_data = trans_df |>
  mutate(across(starts_with("Route"), as.character)) |>
  pivot_longer(cols = starts_with("route"), 
               names_to = "route_Number", 
               values_to = "route_Name", 
               values_drop_na = TRUE)

stations_serving_A = tidy_data |>
  filter(route_Name == "A") |>
  distinct(line, station_name) |>
  nrow()|>
  print()
```

    ## [1] 60

``` r
ada_compliant_stations_A = tidy_data |>
  filter(route_Name == "A" & ada == TRUE) |>
  distinct(line, station_name) |>
  nrow() |>
  print()
```

    ## [1] 17

- 60 distinct stations serve the A train.
- Among them 17 are ADA compliant.

# Problem 2

Read and clean the data

Mr. Trash Wheel

``` r
Trash_wheel = 
  read_excel(
    "data/202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    skip = 1,
    na = c("NA", ",", ".")
  ) |>
  select(-c(...15, ...16)) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(
    sports_balls = as.integer(round(sports_balls))
  )
```

Load Professor Trash Wheel and Gwynnda

``` r
Professor_trash_wheel = 
  read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Professor Trash Wheel",
    skip = 1,
    na = c("NA", ",", ".")
  ) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster))
  

Gwynnda_trash_wheel = 
    read_excel(
    "data/202309 Trash Wheel Collection Data.xlsx", 
    sheet = "Gwynnda Trash Wheel",
    skip = 1,
    na = c("NA", ",", ".")
  ) |>
  janitor::clean_names() |>
  filter(!is.na(dumpster))
```

Combine the datasets

``` r
Trash_wheel = Trash_wheel |> mutate(trash_wheel = "Mr. Trash Wheel",
                      year = as.character(year))
Professor_trash_wheel = Professor_trash_wheel |> mutate(trash_wheel = "Professor Trash Wheel",
                                year = as.character(year))
Gwynnda_trash_wheel = Gwynnda_trash_wheel|>  mutate(trash_wheel = "Gwynnda Trash Wheel",
                              year = as.character(year))


combined_trash_wheel = bind_rows(Trash_wheel, Professor_trash_wheel, Gwynnda_trash_wheel) |>
  select(-c(dumpster)) |>
  relocate(trash_wheel)
```

- The combined dataset of trash collected by Mr. Trash Wheel, Professor
  Trash Wheel, and Gwynnda Trash Wheel contains 912 observations.
- Key variables include weight_tons representing the weight of trash
  collected per dumpster, the time of collection and specific waste
  types like plastic_bottles and cigarette_butts.

``` r
total_weight_prof = sum(
  filter(combined_trash_wheel, trash_wheel == "Professor Trash Wheel") |>
    select(weight_tons),
  na.rm = TRUE
)
total_weight_gwy = sum(
  filter(combined_trash_wheel, trash_wheel == "Professor Trash Wheel" & month == "June" & year == "2022" ) |>
    select(cigarette_butts), 
    na.rm = TRUE
)
```

- For Professor Trash Wheel, the total weight of trash collected is
  216.26 tons. Meanwhile, Gwynnda Trash Wheel collected
  1.16^{4}cigarette butts in June of 2022.

# Problem 3

Import and clean datasets

``` r
bakers_df =   read_csv("data/gbb_datasets/bakers.csv", na = c("NA", "",".")) |>
  janitor::clean_names() |>
  separate(col = baker_name, into = c("baker", "baker_last_name"), sep = " ") 
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = read_csv("data/gbb_datasets/bakes.csv", na = c("NA", "",".", "N/A")) |>
  janitor::clean_names() |>
  mutate(baker = ifelse(baker == '"Jo"', "Joanne", baker))
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df = read_csv("data/gbb_datasets/results.csv", na = c("NA", "","."), skip = 2) |>
  janitor::clean_names()
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
viewers_df = read_csv("data/gbb_datasets/viewers.csv", na = c("NA", "",".")) |>
  janitor::clean_names() |>
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewers",
    names_prefix = "series_"
  ) |>
  arrange(series) |>
  relocate(series, episode)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Check for completeness and correctness

``` r
missing_in_bakes = anti_join(bakers_df, bakes_df, 
                              by = c("baker" = "baker", "series"))

missing_in_results = anti_join(bakers_df, results_df, 
                                by = c("baker" = "baker", "series"))

missing_in_results_for_bakes = anti_join(bakes_df, results_df, 
                                          by = c("baker", "series", "episode"))
```

Create a single dataset

``` r
GBBF_df = left_join(results_df, bakes_df, by = c("series", "episode", "baker")) |>
  left_join(bakers_df, by = c("series", "baker")) |>
  relocate(series, episode, baker, baker_last_name, baker_age, baker_occupation, hometown, signature_bake, technical, result, show_stopper)

write.csv(GBBF_df, file = "data/gbb_datasets/GBBF.csv", row.names = FALSE)
```

Data cleaning process:

- Import datasets, use`Janitor:clean_names` to uniform the column names.
- The `bakes.csv` takes `N/A` as `NA`.
- Check the association of column elements with elements from another
  table，seperate the column `baker name` in `baker.csv` because the
  other datasets only use baker’s first name.
- Use `anti_join` to check the completeness and correctness of the
  dataframes, found that `"Jo"` in `bakes.csv` equals to `Joanne` in
  `results.csv`, use `mutate` to change it.
- Use `left_join` to merge the three processed dataframes. A `left_join`
  means all rows from `results_df` are retained, and matching rows from
  `bakes_df` are added. If no match is found in `bakes_df`, `NA` will be
  introduced for the missing data.
- `Relocate` is used to reorder the columns of the resulting dataset,
  making sure that key columns like series, episode, baker, and related
  information are positioned in a logical order for easier
  interpretation and analysis.
- The final dataset have 1136 rows and 11 columns.

Winner of each episode in Seasons 5 through 10

``` r
winner_df = select(GBBF_df, series, episode, baker, result) |>
  filter(result %in% c("WINNER", "STAR BAKER"), series > 4 ) |>
  print()
```

    ## # A tibble: 60 × 4
    ##    series episode baker   result    
    ##     <dbl>   <dbl> <chr>   <chr>     
    ##  1      5       1 Nancy   STAR BAKER
    ##  2      5       2 Richard STAR BAKER
    ##  3      5       3 Luis    STAR BAKER
    ##  4      5       4 Richard STAR BAKER
    ##  5      5       5 Kate    STAR BAKER
    ##  6      5       6 Chetna  STAR BAKER
    ##  7      5       7 Richard STAR BAKER
    ##  8      5       8 Richard STAR BAKER
    ##  9      5       9 Richard STAR BAKER
    ## 10      5      10 Nancy   WINNER    
    ## # ℹ 50 more rows

Viewership Data

``` r
viewers_df |>
  slice_head(n = 10) |>
  print()
```

    ## # A tibble: 10 × 3
    ##    series episode viewers
    ##    <chr>    <dbl>   <dbl>
    ##  1 1            1    2.24
    ##  2 1            2    3   
    ##  3 1            3    3   
    ##  4 1            4    2.6 
    ##  5 1            5    3.03
    ##  6 1            6    2.75
    ##  7 1            7   NA   
    ##  8 1            8   NA   
    ##  9 1            9   NA   
    ## 10 1           10   NA

``` r
s1_viewer = viewers_df |>
  filter(series == 1) |>
  pull(viewers) |>
  mean(na.rm = TRUE) |>
  print()
```

    ## [1] 2.77

``` r
s5_viewer = viewers_df |>
  filter(series == 5) |>
  pull(viewers) |>
  mean(na.rm = TRUE) |>
  print()
```

    ## [1] 10.0393

The average viewship in Season 1 is 2.77, Season 5 is 10.0393.
